\documentclass[10pt, twocolumn]{article}
\author{Tarang Srivastava}
\usepackage{amsmath, amssymb, amsthm, commath, chngcntr, enumitem, multirow, thmtools, xcolor}
\usepackage{graphicx}
\usepackage[margin=.25in]{geometry}
\setlength{\columnsep}{.5in}
\newcommand{\C}{\mathbb{C}}
\newcommand{\question}[1]{\textcolor{blue}{#1} \\}
\newcommand{\R}{\mathbb{R}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\LinearMap}[2]{\mathcal{L}(#1, #2)}
\newcommand{\poly}[2]{\mathcal{P}_{#1}\left(#2\right)}
\newcommand{\vspan}[1]{\text{span}\left(#1\right)}
\newcommand{\inv}[1]{#1^{-1}}
\newcommand{\todo}[1]{\textcolor{red}{TODO: #1} \\}
\newcommand{\nul}{\text{null }}
\newcommand{\nullity}{\text{nullity }}
\newcommand{\range}{\text{range }}
\newcommand{\rank}{\text{rank }}
\newcommand{\makechaptertitle}[1]{
\begin{center}
	\begin{large}
		#1
	\end{large}
	\begin{small}
		\\Tarang Srivastava
	\end{small}
\end{center}
}
\declaretheoremstyle[
spaceabove=\topsep, spacebelow=\topsep,
headfont=\normalfont\bfseries,
notefont=\bfseries, notebraces={Problem }{},
bodyfont=\normalfont,
postheadspace=0.5em,
name={\ignorespaces},
numbered=no,
headpunct=:]
{mystyle}
\declaretheorem[style=mystyle]{q}

\begin{document}
	
\makechaptertitle{Math 110 Homework 4}

\section{Exercies 3.B}
\begin{q}[7]
    Let $ V $ have dimension 2, and $ W $ have some dimension greater than or equal to 2. 
    Let $ T $ be a non-injective non-zero linear map $ T: V \to W $. 
    We know that $ \nul T $ is a subspace of $ V $. 
    Also, $ \nul T \neq \{0\} $ since it is not injective, 
    thus $ \dim \nul T > 0 $. 
    Since, we restricted $ V $ to have dimension 2 and $ \nul T $ is nonzero it must have exactly dimension 1. 
    There exists a subspace $ U $ of $ V $ such that $ V = \nul T \oplus U $. 
    Therefore, $ \dim U = 1 $.
    Let $ S $ be another  non-zero non-injective linear map $ S : V \to W $ such that $ \nul S = U $. 
    Now we have the case that for $ v \in V $, 
    $$ (S + T)(v) = Sv + Tv $$
    But for any non-zero $ v \in V $ it is either in $ \nul S $ or $ \nul T $ but not both. 
    So, the $ \nul (S + T) = \{0\} $, which means that $ S + T $ is injective. 
    Therefore, it is not a subspace of $ \LinearMap{U}{V} $ since it is not closed under addition.
\end{q}

\begin{q}[15]
    Assume for contradiction that $ T $ is a valid linear map. 
    Given that $ T $ is a map $ T: \R^5 \to \R^2$, we must have that $ \rank T = 2 $ and $ \nullity T = 3 $. 
    By 
    $$ \dim \R^5 = 5 = \rank + \nullity = 2 + 3 $$
    We then observe that the null space defined as follows
    has dimension 2. 
    $$\left\{\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}\right) \in \mathbf{F}^{5}: x_{1}=3 x_{2} \text { and } x_{3}=x_{4}=x_{5}\right\}$$
    We claim that the vectors $ (3, 1, 0, 0, 0), (0, 0, 1, 1, 1) $ span the null space. 
    Observe for some arbitrary coefficients $ a, b \in \F $ the linear combination of the vectors 
    $ a(3, 1, 0, 0, 0) + b(0, 0, 1, 1, 1) = (3a, a, b, b, b)$. 
    This holds the required property that $ 3x_2 = x_1 $ and that $ x_3 = x_4 = x_5 $. 
    Thus, it spans our null space and we have a contradiction, since we found a spanning list that has length less than a linearly independent list.  
\end{q}

\begin{q}[19]
    We know that for a linear map $ T \in \LinearMap{V}{W} $ the dimension of the range is less than or equal to the codomain.
    That is, 
    $$ \rank T \leq \dim W $$
    Suppose, this was not the case then we could find a $ v \in V $ such that $ Tv \not \in W $ and that is not a linear map. 
    Then, we also have that $ \dim \nul T = \dim U $. 
    It then follows directly that 
    $$ \dim V = \rank T + \dim \nul T = \rank T + \dim U $$
    $$ \dim V - \rank T = \dim U $$
    Therefore, from our first inequality 
    $$ \dim U \geq \dim V - \dim W $$
\end{q}

\begin{q}[26]
    Firstly, we are given that $ p $ is a nonconstant polynomial, therefore that implies that $ p $ must have degree greater than 0. 
    So, this satisfies the condition that we will not have negative degrees. 
    Then, we show that for any polynomial $ p $ we can find a polynomial of greater degree by simply multiplying every value by $ x $. 
    So, then all the polynomials of degree greater than 0 can be chosen, and if we have an operator that drops the degree by 1, then it must be the case that 
    the $ \range D = \poly{}{\R} $. So, then it follows that it is surjective by defintion of surjectivity for linear maps.
\end{q}

\section{Exercises 3.C}
\begin{q}[2]
    For $ \poly{3}{\R} $ let the basis be the following basis $ 1, x, x^2, x^3 $ in this order. 
    And for $ \poly{2}{\R} $ let the basis be $ 3x^2, 2x, 1 $ for this order. 
    $ 1, x, x^2, x^3 $ is clearly a basis since it is just the standard basis reorganized, more formally the linear combination will still only be zero for all zero coefficients, since addition is commutative in $ \R $. 
    For  $ 3x^2, 2x, 1 $ any linear combination is only zero when all the coefficients are zero since each term has a different degree. 
\end{q}
\begin{q}[3]
    We want a matrix that basically has 1s in the diagonal up to the $ \rank T $ and 0s everywhere else. 
    So, we begin with the $ \nul T $. 
    We know that $ \nul T $ is a subspace of $ V $. 
    So let $ t_1, ..., t_m $ be a basis for $ \nul T $. 
    Then, since $ t_1, ..., t_m $ is a linearly independent list and is less than or equal to the length of the basis of $ V $ we can extend it to form a basis of $ V $.
    Note that $ \nullity T = m $
    Let $ v_1, ..., v_n $ be the addtional vectors we add to extend our list to be a basis of $ V $. 
    Note that $ \rank T = n $. 
    So, $ t_1, ..., t_m , v_1, ..., v_n $ is a basis of $ V $. 
    Because we collected all the vectors that are equal to zero we know that $ Tv_i = w_i $ for some $ w_i \in W $ for $ i \in \{1, ..., n\} $. 
    Now we can show these $ w_1, ..., w_n $ are linearly independent. 
    We begin with $ a_1w_1 + ... + a_n w_n = 0 $ for some coefficients $ a_i $. 
    We can substitute the $ Tv_i $ to get $ a_1Tv_1 + ... + a_n Tv_n = 0 $. 
    Collecting the terms we have $ T(a_1v_1 + ... + a_n v_n) = 0 $. 
    We know that $ v_1, ..., v_n $ are linearly independent since it is formed by dropping vectors from the basis of $ V $. 
    Then, $ a_1 v_1 + ... + a_n v_n = 0 $ only when all the coefficients are zero.
    So, we know all the $ a_i $ are zero, thus $ w_1, ..., w_n $ is linearly independent. 
    Then we can finally extend $ w_1, ..., w_n $ to form a basis for $ W $. 
    Now we basically our done since we have shown that we have a basis for which $ Tv_i = w_i $ for $ i \in \{1, ..., \rank T\} $. 
    So, we can make our desired matrix since everything else is supposed to be zero because it is in the null space. 
\end{q}
\begin{q}[4]
    In a very roundabout way this question is asking if there is a vector $ v $  in every basis of $ V $ such that for every 
    $ T \in \LinearMap{V}{W} $ it is that $ Tv = w $ for some $ w $ in the basis of $ W $. 
    The stright forward case is if for some $ v $ in the basis of $ V $ if $ Tv = 0 $ then put that $ v $ at the start of the basis list, so that the column can be all zeros and satisfy the condition.
    The second also straight forward case if if for some $ v $ in the basis of $ V $ we happen to have a basis for $ W $ such that $ Tv = w $ for $ w $ in the basis, then we are done as well, by moving both $ v $ and $ w $ to start of their respective basis lists.
    The last case is we can always construct a basis such that the condition is met, by just adding $ Tv = w $ to the basis of $ W $ and then removing all the vectors that are multiples of $ w $. 
    Then we will be left with a linearly independent list that we can always extend to form a basis of $ W $. 
\end{q}

\section{Exercises 3.D}
\begin{q}[3]
    For the $ \implies $ direction if there exists an invertible operator $ T $, such that $ Tu = Su $, then $ S $ is injective since $ T $ is injective.
    For the $ \impliedby $ direction since $ S $ is injective we know that for some basis $ u_1, ..., u_m $ of $ U $ we know $ Su_1, ..., Su_m $ is linearly independent. 
    Then, we can extend $ u_1, ..., u_m $ to form a basis for $ V $. 
    Then select a $ T \in \LinearMap{V}{V} $ such that $ Tu_i = Su_i $ and for the extra vectors $ Tv = v $. 
    We make a direct map for these vectors. 
    Basically, $ Tv = v $. 
    Then, we are guaranteed that $ T $ is injective and therefore also invertible.  
\end{q}

\begin{q}[4]
    For the $ \impliedby $ direction observe that if $ T_1 = ST_2 $, then $ \nul S = \{0\} $. So, essentially all the vectors from $ V $ that mapped to 0, will again map to zero for $ S $.
    So $ \nul T = \nul S $. 
    For the $ \implies $ direction if $ \nul T_1 = \nul T_2 $ then we know there exists a linear operator $ S \in \LinearMap{W}{W} $. 
    We know that for $ v \in \nul T_1, \nul T_2 $ it will map $ T_1v = T_2 v = 0 $. So, then $ T_1 v = ST_2 v $. 
    Since all linear maps map 0 to 0. 
    Suppose, $ u \not \in \nul T_1, \nul T_2 $. Then, define $ S $ such that $ ST_2 = T_1 $.  
    It is straightforward to show that $ S $ is a linear operator since by our construction of $ S $, $ \nul S = \{0\} $, since we require that $ S $ map all nonzero values to a nonzero value. 
    Then, $ S $ is injective and therefore also invertible.
\end{q}
\begin{q}[5]
    For the $ \impliedby $ direction, if we let $ S $ be the identity linear map, which we know is invertible, then $ T_1 = T_2S = T_2 $. So, clearly they are the same and must have the same range.
    For the $ \implies $ direction if $ \range T_1 = \range T_2 $ then we know for some $ v, u \in V $ it is the case that $ T_1v = T_2u $. 
    Then we can define $ S $ such that $ Sv = u $. 
    Then, $ T_1v = T_2Sv $. 
    We now are left to show that such a $ S $ is invertible. 
    Observe, that for all $ u \in V $ there exists a $ v $ such that $ Su = v $. Then, by construction of $ S $ it is surjective. 
    So, it is also invertible.
\end{q}
\begin{q}[7]
    \textbf{(a)}
    For all $ u \in V $ such that $ u \neq v $ we proceed by our normal addition and scalar multiplication for some maps $ T, S \in E $. 
    For $ v $ we have that $ (T+S)(v) = Tv + Sv = 0 + 0 $, so it is closed under addition for all vectors in $ V $. 
    For some $ \lambda \in \F $ it holds that $ T(\lambda v) = \lambda Tv  = \lambda 0 = 0 $, so it satisfies homogeniety as well. Therefore, it is a subspace. \\
    \textbf{(b)} 
    We can start of with $ v $ and construct a basis by extending it for $ V $. 
    But, observe then that the column for $ v $ will have to be all zeros. 
    Therefore, the dimension of $ E $ will be $ E = \dim W (1 - \dim V) $. 
\end{q}
\begin{q}[9]
    For the $ \impliedby $ direction it is straightforward that if $ S $ and $ T $ are both invertible then $ TS\inv{S}\inv{T} = TI\inv{T} = I$. 
    For the other side, $ \inv{T} \inv{S} S T = \inv{T} I T = I $. Therefore, we have found an inverse and it must be invertible.
    For the $ \implies $ direction we can show that $ \nul T = \{0\} $ and then make a similar argument for $ S $. 
    Let $ v \in \nul T $, 
    Then, $ v = \inv{(ST)} (ST)v = 0 $, since after $ Tv $ all the linear maps will map 0 to 0. Therefore,  $ v = 0 $ and we have shown that $ T $ is injective and therefore invertible.
    We proceed with the exact same argument except we select a $ v $ such that $ Tv \in \nul S $. 
    By the same argument as before $ S $ is then injective and thus invertible. 
\end{q}
\begin{q}[10]
    $ ST = I $ implies by definiton that $ S = \inv{T} $. Then, we can take the liear map of $ T $ after each following map and get $ TS = T \inv{T} = I $. 
    We proceed with the exact same argument in the opposite direction to show the if and only if.
\end{q}
\begin{q}[16]
    If $ T $ is a scalar multiple then $ T = \lambda I $. 
    It directly follows then that $ ST = S\lambda I = \lambda I S = TS $. 
    We can make the argument that if there is a $ v $ such that that $ STv = TSv $. Then it must be that if $ u = Sv $. Then, $Tu$ 
    can be considered for $ STu $, but clearly the only linear map that satisfies this is a scalar multiple of the identity map.
\end{q}
\begin{q}[19]
    We know that we can always select a $ p $ such that it has higher degree than $ Tp $ sine we are in infinite dimensions therefore, there for all $ p $ we can always find a $ p' $ such that $ p = Tp' $. 
    Observe that for this reason it is always surjectve. 
    Since, $ \range T = \poly{}{\R} $. 
    For the second part observe that it must be the case that if $ T $ is injective then it must also be invertible therefore $ \inv{T}T p = p $. 
    Thus, there exists no operation such that you can decrease the degree and still be surjective, so it must be that for all nonzero $ p $ 
    the statement holds that $ deg Tp = deg p $. 
\end{q}

\section{Exercises 3.E}
\begin{q}[7]
    We know from the definition that $ v + U = x + U $, so if we now given that $ v + U = x + U = x + W $ then it clearly follows that $ U = W $. 
\end{q}
\begin{q}[8]
    Observe for the $ \impliedby $ direction that we begin with $ \lambda v + (1 - \lambda)w $. 
    We can then do the following algebraic manipulations to get that $ \lambda(v-w) + w \in A $ which is clearly the case and then it must be that it is affine for all $ v, w \in A $ since it is closed, and follows from that. 
    For the $ \implies $ direction consider that for an affine subset of $ V $ we have that for all $ v $ and $ w $ as in the argument above we can just make the same manipulations in the other direction and show the statement holds.
\end{q}
\begin{q}[9]
    For an affine subsets $ A_1 $ and $ A_2$  it is parrallel to $ V $ then it must be that the intersection of two affine subsets that are both parrallel to $ V $ are parrallel to each other. 
    Therefore, and intersection will also be parrallel to $ V $ or simply not exist in which case be the empty set. Therefore we have that $A_1 \cap A_2 $ is an affine subset as well (or the empty set). 
\end{q}

\end{document}